# üìö LearnLoop ‚Äì Your Personal Offline AI Tutor

**LearnLoop** is a private, offline, intelligent tutor that helps you learn smarter ‚Äî no APIs, no cloud, no tracking. Powered by open-source LLMs like Mistral or LLaMA, it adapts to your learning style, remembers your questions, and provides contextual, real-time answers.

> ‚ú® 100% Local. Fully Private. Internet-Free AI Tutoring.

---

![screenshot](./screenshot.png)

---

## üåü Key Highlights

‚úÖ **Runs 100% Locally** ‚Äî No OpenAI, no API keys  
‚úÖ **Learns Over Time** ‚Äî Personalized with memory via ChromaDB  
‚úÖ **Fast, Responsive UI** ‚Äî Built with Next.js + Tailwind CSS  
‚úÖ **Powered by Ollama** ‚Äî Easily run Mistral or LLaMA models locally  
‚úÖ **Built for Hackathons** ‚Äî Lightweight, simple, and fast to deploy

---

## üñ•Ô∏è Web App Overview

| Layer        | Technology Used              |
|--------------|------------------------------|
| üåê Frontend  | Next.js + Tailwind CSS        |
| ‚öôÔ∏è Backend   | FastAPI (Python)              |
| üß† AI Engine | Ollama (Local LLM: Mistral)   |
| üóÇÔ∏è Memory    | ChromaDB (Vector DB)          |

---

## üöÄ Run the App Locally (in 4 Steps)

> üõë No internet or cloud services needed after setup.

### 1Ô∏è‚É£ Clone the Repo
```bash``
git clone https://github.com/Tamannapanwar17/smart-tutor-ai.git
cd smart-tutor-ai
cd backend
python -m venv venv
# Windows:
venv\Scripts\activate
# macOS/Linux:
source venv/bin/activate

pip install -r requirements.txt
uvicorn app:app --reload  # Backend runs at http://localhost:8000
cd ../frontend
npm install
npm run dev  # Frontend runs at http://localhost:3000
# One-time install if not done:
curl -fsSL https://ollama.com/install.sh | sh

# Then run the model:
ollama run mistral  # Starts LLM at http://localhost:11434
git init
git add .
git commit -m "Initial commit for LearnLoop"
gh repo create smart-tutor-ai --public --source=. --push
